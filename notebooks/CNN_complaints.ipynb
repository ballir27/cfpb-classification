{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aPUEBacNJSQm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "texts = []\n",
        "labels_ = []\n",
        "\n",
        "# Open the CSV file and read it line by line\n",
        "with open(\"/content/complaints.csv\", \"r\", encoding=\"utf-8\") as file:\n",
        "    # Create a CSV reader\n",
        "    csv_reader = csv.reader(file)\n",
        "    # Skip the header line\n",
        "    next(csv_reader)\n",
        "    # Iterate over each row in the CSV file\n",
        "    for row in csv_reader:\n",
        "      # Extract text and label from the row\n",
        "      text = row[0]  # Remove leading and trailing quotes\n",
        "      if len(row) > 1:\n",
        "        label = row[1]  # Extract label from the second column\n",
        "      else:\n",
        "        label = None\n",
        "      # Append text and label to the respective lists\n",
        "      texts.append(text)\n",
        "      labels_.append(label)"
      ],
      "metadata": {
        "id": "3ViH4dJuOJio"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "H9UKrRO-PCBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "255f7f5f-007f-4619-f084-516d4072a319"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocess the text data\n",
        "texts = [word_tokenize(text.lower()) for text in texts]\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels_)\n"
      ],
      "metadata": {
        "id": "I5gabmbqKZyj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert texts to sequences and pad them\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "-_rOg8WmfEMR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "embedding_dim = 100\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_sequence_length),\n",
        "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "34reoUuLdOrq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a551b52-927c-48d9-d742-147e78975b9c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3535/3535 [==============================] - 196s 54ms/step - loss: 0.3494 - accuracy: 0.8714 - val_loss: 0.2946 - val_accuracy: 0.8906\n",
            "Epoch 2/10\n",
            "3535/3535 [==============================] - 82s 23ms/step - loss: 0.2434 - accuracy: 0.9102 - val_loss: 0.3010 - val_accuracy: 0.8863\n",
            "Epoch 3/10\n",
            "3535/3535 [==============================] - 71s 20ms/step - loss: 0.1790 - accuracy: 0.9352 - val_loss: 0.2993 - val_accuracy: 0.8934\n",
            "Epoch 4/10\n",
            "3535/3535 [==============================] - 68s 19ms/step - loss: 0.1237 - accuracy: 0.9563 - val_loss: 0.3534 - val_accuracy: 0.8840\n",
            "Epoch 5/10\n",
            "3535/3535 [==============================] - 65s 18ms/step - loss: 0.0857 - accuracy: 0.9706 - val_loss: 0.3906 - val_accuracy: 0.8921\n",
            "Epoch 6/10\n",
            "3535/3535 [==============================] - 64s 18ms/step - loss: 0.0637 - accuracy: 0.9784 - val_loss: 0.4471 - val_accuracy: 0.8895\n",
            "Epoch 7/10\n",
            "3535/3535 [==============================] - 64s 18ms/step - loss: 0.0506 - accuracy: 0.9829 - val_loss: 0.5206 - val_accuracy: 0.8813\n",
            "Epoch 8/10\n",
            "3535/3535 [==============================] - 64s 18ms/step - loss: 0.0432 - accuracy: 0.9852 - val_loss: 0.5217 - val_accuracy: 0.8845\n",
            "Epoch 9/10\n",
            "3535/3535 [==============================] - 63s 18ms/step - loss: 0.0368 - accuracy: 0.9875 - val_loss: 0.5453 - val_accuracy: 0.8888\n",
            "Epoch 10/10\n",
            "3535/3535 [==============================] - 62s 18ms/step - loss: 0.0333 - accuracy: 0.9887 - val_loss: 0.6073 - val_accuracy: 0.8906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "IP5hi-flbjFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59800d3a-4b88-4f66-ea25-d0271d1da8fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2209/2209 [==============================] - 8s 3ms/step - loss: 0.6092 - accuracy: 0.8914\n",
            "Test Loss: 0.6091961860656738, Test Accuracy: 0.891366183757782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the probabilities for the test data\n",
        "y_pred_prob = model.predict(X_test)\n",
        "\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "# Convert encoded labels back to original labels\n",
        "y_test_original = label_encoder.inverse_transform(y_test)\n",
        "y_pred_original = label_encoder.inverse_transform(y_pred)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test_original, y_pred_original)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "qZOVqal0beCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d92fa1-ae38-4b0c-e396-bb4ae5f7ba62"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2209/2209 [==============================] - 6s 3ms/step\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "   Attempts to collect debt not owed       0.77      0.76      0.77     14477\n",
            "               Communication tactics       0.83      0.72      0.77      4160\n",
            "                       Fraud or scam       0.93      0.83      0.88      2391\n",
            "Incorrect information on your report       0.93      0.95      0.94     46122\n",
            "          Struggling to pay mortgage       0.89      0.94      0.91      3537\n",
            "\n",
            "                            accuracy                           0.89     70687\n",
            "                           macro avg       0.87      0.84      0.85     70687\n",
            "                        weighted avg       0.89      0.89      0.89     70687\n",
            "\n"
          ]
        }
      ]
    }
  ]
}