{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPUEBacNJSQm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ViH4dJuOJio"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "texts = []\n",
        "labels_ = []\n",
        "\n",
        "# Open the CSV file and read it line by line\n",
        "with open(\"/content/complaints.csv\", \"r\", encoding=\"utf-8\") as file:\n",
        "    # Create a CSV reader\n",
        "    csv_reader = csv.reader(file)\n",
        "    # Skip the header line\n",
        "    next(csv_reader)\n",
        "    # Iterate over each row in the CSV file\n",
        "    for row in csv_reader:\n",
        "      # Extract text and label from the row\n",
        "      text = row[0]  # Remove leading and trailing quotes\n",
        "      if len(row) > 1:\n",
        "        label = row[1]  # Extract label from the second column\n",
        "      else:\n",
        "        label = None\n",
        "      # Append text and label to the respective lists\n",
        "      texts.append(text)\n",
        "      labels_.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9UKrRO-PCBA",
        "outputId": "1193632a-cafe-4358-e996-a83abe02ef4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5gabmbqKZyj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Preprocess the text data\n",
        "texts = [word_tokenize(text.lower()) for text in texts]\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_rOg8WmfEMR"
      },
      "outputs": [],
      "source": [
        "# Convert texts to sequences and pad them\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34reoUuLdOrq",
        "outputId": "169a5718-f594-47be-a1cb-00ee395c2cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "90/90 [==============================] - 23s 207ms/step - loss: 0.9602 - accuracy: 0.6894 - val_loss: 0.7687 - val_accuracy: 0.7088\n",
            "Epoch 2/10\n",
            "90/90 [==============================] - 17s 193ms/step - loss: 0.5978 - accuracy: 0.7783 - val_loss: 0.5449 - val_accuracy: 0.7926\n",
            "Epoch 3/10\n",
            "90/90 [==============================] - 14s 157ms/step - loss: 0.3815 - accuracy: 0.8705 - val_loss: 0.4504 - val_accuracy: 0.8373\n",
            "Epoch 4/10\n",
            "90/90 [==============================] - 14s 157ms/step - loss: 0.2220 - accuracy: 0.9312 - val_loss: 0.4444 - val_accuracy: 0.8506\n",
            "Epoch 5/10\n",
            "90/90 [==============================] - 14s 162ms/step - loss: 0.1129 - accuracy: 0.9701 - val_loss: 0.4805 - val_accuracy: 0.8422\n",
            "Epoch 6/10\n",
            "90/90 [==============================] - 15s 161ms/step - loss: 0.0463 - accuracy: 0.9925 - val_loss: 0.5345 - val_accuracy: 0.8485\n",
            "Epoch 7/10\n",
            "90/90 [==============================] - 11s 120ms/step - loss: 0.0233 - accuracy: 0.9967 - val_loss: 0.5559 - val_accuracy: 0.8436\n",
            "Epoch 8/10\n",
            "90/90 [==============================] - 10s 108ms/step - loss: 0.0148 - accuracy: 0.9976 - val_loss: 0.5835 - val_accuracy: 0.8506\n",
            "Epoch 9/10\n",
            "90/90 [==============================] - 10s 109ms/step - loss: 0.0131 - accuracy: 0.9984 - val_loss: 0.5988 - val_accuracy: 0.8492\n",
            "Epoch 10/10\n",
            "90/90 [==============================] - 11s 120ms/step - loss: 0.0078 - accuracy: 0.9984 - val_loss: 0.6188 - val_accuracy: 0.8450\n"
          ]
        }
      ],
      "source": [
        "# Define model parameters\n",
        "embedding_dim = 100\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_sequence_length),\n",
        "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP5hi-flbjFt",
        "outputId": "3bd309eb-5e50-4b35-e7b7-9b77a5bb8454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56/56 [==============================] - 1s 12ms/step - loss: 0.5627 - accuracy: 0.8520\n",
            "Test Loss: 0.5627123713493347, Test Accuracy: 0.8520379662513733\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZOVqal0beCU",
        "outputId": "b061e913-75d7-43a6-a239-de64b0ea5493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56/56 [==============================] - 0s 7ms/step\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "   Attempts to collect debt not owed       0.68      0.73      0.71       379\n",
            "               Communication tactics       0.60      0.59      0.60        61\n",
            "                       Fraud or scam       0.73      0.86      0.79        43\n",
            "Incorrect information on your report       0.93      0.90      0.91      1230\n",
            "          Struggling to pay mortgage       0.86      0.83      0.84        78\n",
            "\n",
            "                            accuracy                           0.85      1791\n",
            "                           macro avg       0.76      0.78      0.77      1791\n",
            "                        weighted avg       0.86      0.85      0.85      1791\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Predict the probabilities for the test data\n",
        "y_pred_prob = model.predict(X_test)\n",
        "\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "# Convert encoded labels back to original labels\n",
        "y_test_original = label_encoder.inverse_transform(y_test)\n",
        "y_pred_original = label_encoder.inverse_transform(y_pred)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test_original, y_pred_original)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "conv_layers = [layer for layer in model.layers if isinstance(layer, tf.keras.layers.Conv1D)]\n",
        "\n",
        "# Visualize filters for each convolutional layer\n",
        "for i, layer in enumerate(conv_layers):\n",
        "    filters = layer.get_weights()[0]\n",
        "    num_filters = filters.shape[2]\n",
        "    \n",
        "    # Plot each filter as a grid\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for j in range(num_filters):\n",
        "        plt.subplot(num_filters/8, 8, j+1)\n",
        "        plt.plot(filters[:, :, j])\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(f'Conv1D Layer {i+1}')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
