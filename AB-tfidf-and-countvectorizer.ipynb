{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9145af-ffa7-40bb-bbd0-78828f5c569e",
   "metadata": {},
   "source": [
    "***Classification of Consumer Complaints***\n",
    "\n",
    "The Consumer Financial Protection Bureau publishes the Consumer Complaint Database, a collection of complaints about consumer financial products and services that were sent to companies for response. Complaints are published after the company responds, confirming a commercial relationship with the consumer, or after 15 days, whichever comes first.\n",
    "\n",
    "You have been provided with a dataset of over 350,000 such complaints for 5 common issue types. Your goal is to train a text classification model to identify the issue type based on the consumer complaint narrative. The data can be downloaded from https://drive.google.com/file/d/1Hz1gnCCr-SDGjnKgcPbg7Nd3NztOLdxw/view?usp=share_link\n",
    "\n",
    "At the end of the project, your team should should prepare a short presentation where you talk about the following:\n",
    "\n",
    "* What steps did you take to preprocess the data?\n",
    "* How did a model using unigrams compare to one using bigrams or trigrams?\n",
    "* How did a count vectorizer compare to a tfidf vectorizer?\n",
    "* What models did you try and how successful were they? Where did they struggle? Were there issues that the models commonly mixed up?\n",
    "* What words or phrases were most influential on your models' predictions?\n",
    "\n",
    "Bonus: A larger dataset containing 20 additional categories can be downloaded from https://drive.google.com/file/d/1gW6LScUL-Z7mH6gUZn-1aNzm4p4CvtpL/view?usp=share_link. How well do your models work with these additional categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8eaaff81-d6a6-4d17-8722-5f860bc65449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22475712-77c1-4146-aaed-ca14d2b26df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints = pd.read_csv('data/complaints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a90847-e969-4bf5-8dd7-ce4fd8cb4568",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints=complaints.rename(columns={'Consumer complaint narrative':'narrative', 'Issue':'issue'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1fad4e6-47b0-440a-818a-57573fb151e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>narrative</th>\n",
       "      <th>issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My name is XXXX XXXX this complaint is not mad...</td>\n",
       "      <td>Incorrect information on your report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I searched on XXXX for XXXXXXXX XXXX  and was ...</td>\n",
       "      <td>Fraud or scam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have a particular account that is stating th...</td>\n",
       "      <td>Incorrect information on your report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have not supplied proof under the doctrine o...</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello i'm writing regarding account on my cred...</td>\n",
       "      <td>Incorrect information on your report</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           narrative  \\\n",
       "0  My name is XXXX XXXX this complaint is not mad...   \n",
       "1  I searched on XXXX for XXXXXXXX XXXX  and was ...   \n",
       "2  I have a particular account that is stating th...   \n",
       "3  I have not supplied proof under the doctrine o...   \n",
       "4  Hello i'm writing regarding account on my cred...   \n",
       "\n",
       "                                  issue  \n",
       "0  Incorrect information on your report  \n",
       "1                         Fraud or scam  \n",
       "2  Incorrect information on your report  \n",
       "3     Attempts to collect debt not owed  \n",
       "4  Incorrect information on your report  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1046f3b-187d-437b-bc31-6a502658393f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "issue\n",
       "Incorrect information on your report    229305\n",
       "Attempts to collect debt not owed        73163\n",
       "Communication tactics                    21243\n",
       "Struggling to pay mortgage               17374\n",
       "Fraud or scam                            12347\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints['issue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f48b6fc4-3496-48c8-9479-12a058c3a7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>narrative</th>\n",
       "      <th>issue</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My name is XXXX XXXX this complaint is not mad...</td>\n",
       "      <td>Incorrect information on your report</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I searched on XXXX for XXXXXXXX XXXX  and was ...</td>\n",
       "      <td>Fraud or scam</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have a particular account that is stating th...</td>\n",
       "      <td>Incorrect information on your report</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have not supplied proof under the doctrine o...</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello i'm writing regarding account on my cred...</td>\n",
       "      <td>Incorrect information on your report</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           narrative  \\\n",
       "0  My name is XXXX XXXX this complaint is not mad...   \n",
       "1  I searched on XXXX for XXXXXXXX XXXX  and was ...   \n",
       "2  I have a particular account that is stating th...   \n",
       "3  I have not supplied proof under the doctrine o...   \n",
       "4  Hello i'm writing regarding account on my cred...   \n",
       "\n",
       "                                  issue  category  \n",
       "0  Incorrect information on your report         1  \n",
       "1                         Fraud or scam         5  \n",
       "2  Incorrect information on your report         1  \n",
       "3     Attempts to collect debt not owed         2  \n",
       "4  Incorrect information on your report         1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints['category'] = 5\n",
    "\n",
    "complaints['category'] = np.where(complaints['issue'] == 'Incorrect information on your report', 1, complaints['category'])\n",
    "complaints['category'] = np.where(complaints['issue'] == 'Attempts to collect debt not owed', 2, complaints['category'])\n",
    "complaints['category'] = np.where(complaints['issue'] == 'Communication tactics', 3, complaints['category'])\n",
    "complaints['category'] = np.where(complaints['issue'] == 'Struggling to pay mortgage', 4, complaints['category'])\n",
    "\n",
    "\n",
    "#def categorize_issues(complaints_df):\n",
    "    # if 'Incorrect information on your report' in issue:\n",
    "    #     return '1'\n",
    "    # elif 'Attempts to collect debt not owed' in issue:\n",
    "    #     return '2'\n",
    "    # elif 'Communication tactics' in issue:\n",
    "    #     return '3'\n",
    "    # elif 'Struggling to pay mortgage' in issue:\n",
    "    #     return '4'\n",
    "    # else:\n",
    "    #     return '5'\n",
    "#complaints['category'] = complaints['issue'].apply(categorize_issue)\n",
    "\n",
    "complaints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dddc3b1f-2312-4eac-bc9b-3d53a929b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = complaints[['narrative']]\n",
    "y = complaints['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 321, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27d1f2ad-9a13-45e4-bcd2-26b3eb9a54df",
   "metadata": {},
   "outputs": [],
   "source": [
    "countvect = CountVectorizer(stop_words = 'english')\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "clf = MultinomialNB()\n",
    "\n",
    "pipe = Pipeline([(\"vect\", countvect), (\"clf\", clf)])\n",
    "\n",
    "param_grid = [{\n",
    "    'vect': [countvect],\n",
    "    'vect__ngram_range':[(1,1), (1,2), (1,3)],\n",
    "    'vect__min_df':[1,2,5,10]\n",
    "},\n",
    "              {\n",
    "    'vect': [tfidf],\n",
    "    'vect__ngram_range':[(1,1), (1,2), (1,3)],\n",
    "    'vect__min_df':[1,2,5,10]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a8370ea-0b2d-4e0a-941b-725ae0a3b6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "The SearchCV fit took 3098.6729254722595 seconds to run\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/cv_01.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "gs = GridSearchCV(estimator = pipe, param_grid = param_grid, verbose = 3, n_jobs = 2, cv=3)\n",
    "gs.fit(X_train['narrative'], y_train)\n",
    "end = time.time()\n",
    "\n",
    "print(f'The SearchCV fit took {end - start} seconds to run')\n",
    "\n",
    "dump(gs, \"data/cv_01.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4469505-624a-454a-a7ec-ff3fc8a08044",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = load(\"data/cv_01.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58ea8b96-79b3-485e-a718-68f87dcd9ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8599334525453269\n",
      "[[51972  4376    45   836    97]\n",
      " [ 3803 13813   413   177    85]\n",
      " [  208  1536  3428   121    18]\n",
      " [   77    47    17  4199     3]\n",
      " [  239   223    10    45  2570]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.91      0.91     57326\n",
      "           2       0.69      0.76      0.72     18291\n",
      "           3       0.88      0.65      0.74      5311\n",
      "           4       0.78      0.97      0.86      4343\n",
      "           5       0.93      0.83      0.88      3087\n",
      "\n",
      "    accuracy                           0.86     88358\n",
      "   macro avg       0.84      0.82      0.82     88358\n",
      "weighted avg       0.87      0.86      0.86     88358\n",
      "\n",
      "1 = Incorrect information on your report\n",
      "2 = Attempts to collect debt not owed\n",
      "3 = Communication tactics\n",
      "4 = Struggling to pay mortgage\n",
      "5 = Fraud or scam\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs.best_estimator_.predict(X_test['narrative'])\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('1 = Incorrect information on your report')\n",
    "print('2 = Attempts to collect debt not owed')\n",
    "print('3 = Communication tactics')\n",
    "print('4 = Struggling to pay mortgage')\n",
    "print('5 = Fraud or scam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4acbe0e-5705-42a5-8f66-36bfdaa4777c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of true predictions: 75982\n",
      "Number of false predictions: 12376\n"
     ]
    }
   ],
   "source": [
    "true_predictions=y_test==y_pred\n",
    "false_predictions=y_test!=y_pred\n",
    "\n",
    "print('Number of true predictions:', np.sum(true_predictions))\n",
    "print('Number of false predictions:', np.sum(false_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a72777db-5bdd-4555-8323-1bc22b723a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 CountVectorizer(ngram_range=(1, 2), stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 CountVectorizer(ngram_range=(1, 2), stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(1, 2), stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(ngram_range=(1, 2), stop_words='english')),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83a6be04-32b8-4626-958e-6ccd9cb584f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 25.73365831,  56.93866404, 109.16719619,  25.97839467,\n",
       "         47.55933364,  82.09299707,  26.38311195,  50.85412439,\n",
       "         78.34353614,  28.57733226,  55.32667184,  77.11768762,\n",
       "         25.66332857,  59.98081748, 121.49669592,  27.14179699,\n",
       "         53.36714824,  91.75572832,  27.77487214,  52.70489375,\n",
       "         83.43306629,  26.17602539,  53.21849759,  77.07787704]),\n",
       " 'std_fit_time': array([ 1.2988156 ,  0.05600926,  3.17649889,  1.73350263,  0.73601773,\n",
       "         2.09475262,  0.40586041,  0.43967035,  1.48870247,  3.61767182,\n",
       "        11.27289565,  0.93642913,  1.21720076,  0.47828642,  1.85872762,\n",
       "         2.16114216,  2.72111761,  1.47505455,  1.04442195,  1.68790887,\n",
       "         1.22457008,  1.07979695,  1.2892584 ,  1.9615998 ]),\n",
       " 'mean_score_time': array([13.08966788, 25.44933009, 37.73933291, 13.53366661, 22.75434454,\n",
       "        36.14643741, 12.88911915, 23.61010583, 33.78799884, 17.52335397,\n",
       "        22.47432184, 31.42031058, 12.71233416, 27.27310475, 42.63567432,\n",
       "        13.50502308, 25.09024429, 40.76343298, 13.71234663, 25.23065988,\n",
       "        35.46846684, 14.03628198, 24.04619805, 31.60730441]),\n",
       " 'std_score_time': array([0.3363938 , 2.17470956, 1.78177081, 0.68683389, 1.10410771,\n",
       "        0.75974497, 0.29653303, 1.21946032, 0.39583628, 4.97787726,\n",
       "        1.53219487, 1.07097654, 0.3394948 , 0.58228067, 1.44888753,\n",
       "        0.38877306, 1.95988684, 1.24179476, 0.35976695, 0.67025507,\n",
       "        0.85445975, 0.55336348, 0.06177182, 1.33269994]),\n",
       " 'param_vect': masked_array(data=[CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "                    CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "                    TfidfVectorizer(stop_words='english'),\n",
       "                    TfidfVectorizer(stop_words='english'),\n",
       "                    TfidfVectorizer(stop_words='english'),\n",
       "                    TfidfVectorizer(stop_words='english'),\n",
       "                    TfidfVectorizer(stop_words='english'),\n",
       "                    TfidfVectorizer(stop_words='english'),\n",
       "                    TfidfVectorizer(stop_words='english'),\n",
       "                    TfidfVectorizer(stop_words='english'),\n",
       "                    TfidfVectorizer(stop_words='english'),\n",
       "                    TfidfVectorizer(stop_words='english'),\n",
       "                    TfidfVectorizer(stop_words='english'),\n",
       "                    TfidfVectorizer(stop_words='english')],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__min_df': masked_array(data=[1, 1, 1, 2, 2, 2, 5, 5, 5, 10, 10, 10, 1, 1, 1, 2, 2,\n",
       "                    2, 5, 5, 5, 10, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vect__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 3), (1, 1), (1, 2), (1, 3), (1, 1),\n",
       "                    (1, 2), (1, 3), (1, 1), (1, 2), (1, 3), (1, 1), (1, 2),\n",
       "                    (1, 3), (1, 1), (1, 2), (1, 3), (1, 1), (1, 2), (1, 3),\n",
       "                    (1, 1), (1, 2), (1, 3)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "   'vect__min_df': 1,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "   'vect__min_df': 1,\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "   'vect__min_df': 1,\n",
       "   'vect__ngram_range': (1, 3)},\n",
       "  {'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "   'vect__min_df': 2,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "   'vect__min_df': 2,\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "   'vect__min_df': 2,\n",
       "   'vect__ngram_range': (1, 3)},\n",
       "  {'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "   'vect__min_df': 5,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "   'vect__min_df': 5,\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "   'vect__min_df': 5,\n",
       "   'vect__ngram_range': (1, 3)},\n",
       "  {'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "   'vect__min_df': 10,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "   'vect__min_df': 10,\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'vect': CountVectorizer(ngram_range=(1, 2), stop_words='english'),\n",
       "   'vect__min_df': 10,\n",
       "   'vect__ngram_range': (1, 3)},\n",
       "  {'vect': TfidfVectorizer(stop_words='english'),\n",
       "   'vect__min_df': 1,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'vect': TfidfVectorizer(stop_words='english'),\n",
       "   'vect__min_df': 1,\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'vect': TfidfVectorizer(stop_words='english'),\n",
       "   'vect__min_df': 1,\n",
       "   'vect__ngram_range': (1, 3)},\n",
       "  {'vect': TfidfVectorizer(stop_words='english'),\n",
       "   'vect__min_df': 2,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'vect': TfidfVectorizer(stop_words='english'),\n",
       "   'vect__min_df': 2,\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'vect': TfidfVectorizer(stop_words='english'),\n",
       "   'vect__min_df': 2,\n",
       "   'vect__ngram_range': (1, 3)},\n",
       "  {'vect': TfidfVectorizer(stop_words='english'),\n",
       "   'vect__min_df': 5,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'vect': TfidfVectorizer(stop_words='english'),\n",
       "   'vect__min_df': 5,\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'vect': TfidfVectorizer(stop_words='english'),\n",
       "   'vect__min_df': 5,\n",
       "   'vect__ngram_range': (1, 3)},\n",
       "  {'vect': TfidfVectorizer(stop_words='english'),\n",
       "   'vect__min_df': 10,\n",
       "   'vect__ngram_range': (1, 1)},\n",
       "  {'vect': TfidfVectorizer(stop_words='english'),\n",
       "   'vect__min_df': 10,\n",
       "   'vect__ngram_range': (1, 2)},\n",
       "  {'vect': TfidfVectorizer(stop_words='english'),\n",
       "   'vect__min_df': 10,\n",
       "   'vect__ngram_range': (1, 3)}],\n",
       " 'split0_test_score': array([0.81159601, 0.85830372, 0.84396433, 0.80433011, 0.8375812 ,\n",
       "        0.8397655 , 0.80120646, 0.80898164, 0.79304647, 0.79947486,\n",
       "        0.79598904, 0.77242581, 0.79168836, 0.67510582, 0.6698205 ,\n",
       "        0.82909301, 0.72576337, 0.7062292 , 0.84373798, 0.82611648,\n",
       "        0.81256932, 0.84628443, 0.85736436, 0.85162634]),\n",
       " 'split1_test_score': array([0.81037371, 0.85639105, 0.84475656, 0.80462437, 0.83689083,\n",
       "        0.83955047, 0.80143281, 0.81037371, 0.79400847, 0.80000679,\n",
       "        0.79704158, 0.7730596 , 0.79043211, 0.67544535, 0.66972996,\n",
       "        0.8279273 , 0.72501641, 0.70770049, 0.84199507, 0.82611648,\n",
       "        0.81396138, 0.84506213, 0.85588175, 0.85049458]),\n",
       " 'split2_test_score': array([0.81280699, 0.85738699, 0.84391906, 0.80651441, 0.84037665,\n",
       "        0.84347767, 0.80285882, 0.81342946, 0.79790172, 0.80188551,\n",
       "        0.80065189, 0.77657937, 0.78984359, 0.67510582, 0.67000158,\n",
       "        0.82888929, 0.72374884, 0.70573123, 0.84390774, 0.82552797,\n",
       "        0.81098486, 0.84686163, 0.85648159, 0.85144526]),\n",
       " 'mean_test_score': array([0.81159223, 0.85736059, 0.84421331, 0.8051563 , 0.83828289,\n",
       "        0.84093121, 0.8018327 , 0.81092827, 0.79498555, 0.80045572,\n",
       "        0.79789417, 0.77402159, 0.79065469, 0.675219  , 0.66985068,\n",
       "        0.82863653, 0.72484287, 0.70655364, 0.84321359, 0.82592031,\n",
       "        0.81250519, 0.8460694 , 0.8565759 , 0.85118872]),\n",
       " 'std_test_score': array([0.00099339, 0.00078107, 0.00038458, 0.00096781, 0.0015071 ,\n",
       "        0.00180276, 0.00073144, 0.00185767, 0.00209911, 0.00103407,\n",
       "        0.00199678, 0.00182704, 0.00076939, 0.00016005, 0.00011292,\n",
       "        0.00050835, 0.00083153, 0.00083604, 0.00086441, 0.00027743,\n",
       "        0.00121601, 0.00075021, 0.00060893, 0.00049637]),\n",
       " 'rank_test_score': array([12,  1,  5, 14,  8,  7, 15, 13, 18, 16, 17, 20, 19, 23, 24,  9, 21,\n",
       "        22,  6, 10, 11,  4,  2,  3])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1ae1acb-dae8-4639-9723-c046eba6726f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vect</th>\n",
       "      <th>vect__min_df</th>\n",
       "      <th>vect__ngram_range</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2), stop_words...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.811592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2), stop_words...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.857361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2), stop_words...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.844213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2), stop_words...</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.805156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2), stop_words...</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.838283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2), stop_words...</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.840931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2), stop_words...</td>\n",
       "      <td>5</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.801833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2), stop_words...</td>\n",
       "      <td>5</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.810928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2), stop_words...</td>\n",
       "      <td>5</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.794986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2), stop_words...</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.800456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2), stop_words...</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.797894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2), stop_words...</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.774022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TfidfVectorizer(stop_words='english')</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.790655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TfidfVectorizer(stop_words='english')</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.675219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TfidfVectorizer(stop_words='english')</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.669851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TfidfVectorizer(stop_words='english')</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.828637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TfidfVectorizer(stop_words='english')</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.724843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TfidfVectorizer(stop_words='english')</td>\n",
       "      <td>2</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.706554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TfidfVectorizer(stop_words='english')</td>\n",
       "      <td>5</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.843214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TfidfVectorizer(stop_words='english')</td>\n",
       "      <td>5</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.825920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TfidfVectorizer(stop_words='english')</td>\n",
       "      <td>5</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.812505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TfidfVectorizer(stop_words='english')</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.846069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TfidfVectorizer(stop_words='english')</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.856576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TfidfVectorizer(stop_words='english')</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.851189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 vect  vect__min_df  \\\n",
       "0   CountVectorizer(ngram_range=(1, 2), stop_words...             1   \n",
       "1   CountVectorizer(ngram_range=(1, 2), stop_words...             1   \n",
       "2   CountVectorizer(ngram_range=(1, 2), stop_words...             1   \n",
       "3   CountVectorizer(ngram_range=(1, 2), stop_words...             2   \n",
       "4   CountVectorizer(ngram_range=(1, 2), stop_words...             2   \n",
       "5   CountVectorizer(ngram_range=(1, 2), stop_words...             2   \n",
       "6   CountVectorizer(ngram_range=(1, 2), stop_words...             5   \n",
       "7   CountVectorizer(ngram_range=(1, 2), stop_words...             5   \n",
       "8   CountVectorizer(ngram_range=(1, 2), stop_words...             5   \n",
       "9   CountVectorizer(ngram_range=(1, 2), stop_words...            10   \n",
       "10  CountVectorizer(ngram_range=(1, 2), stop_words...            10   \n",
       "11  CountVectorizer(ngram_range=(1, 2), stop_words...            10   \n",
       "12              TfidfVectorizer(stop_words='english')             1   \n",
       "13              TfidfVectorizer(stop_words='english')             1   \n",
       "14              TfidfVectorizer(stop_words='english')             1   \n",
       "15              TfidfVectorizer(stop_words='english')             2   \n",
       "16              TfidfVectorizer(stop_words='english')             2   \n",
       "17              TfidfVectorizer(stop_words='english')             2   \n",
       "18              TfidfVectorizer(stop_words='english')             5   \n",
       "19              TfidfVectorizer(stop_words='english')             5   \n",
       "20              TfidfVectorizer(stop_words='english')             5   \n",
       "21              TfidfVectorizer(stop_words='english')            10   \n",
       "22              TfidfVectorizer(stop_words='english')            10   \n",
       "23              TfidfVectorizer(stop_words='english')            10   \n",
       "\n",
       "   vect__ngram_range  mean_test_score  \n",
       "0             (1, 1)         0.811592  \n",
       "1             (1, 2)         0.857361  \n",
       "2             (1, 3)         0.844213  \n",
       "3             (1, 1)         0.805156  \n",
       "4             (1, 2)         0.838283  \n",
       "5             (1, 3)         0.840931  \n",
       "6             (1, 1)         0.801833  \n",
       "7             (1, 2)         0.810928  \n",
       "8             (1, 3)         0.794986  \n",
       "9             (1, 1)         0.800456  \n",
       "10            (1, 2)         0.797894  \n",
       "11            (1, 3)         0.774022  \n",
       "12            (1, 1)         0.790655  \n",
       "13            (1, 2)         0.675219  \n",
       "14            (1, 3)         0.669851  \n",
       "15            (1, 1)         0.828637  \n",
       "16            (1, 2)         0.724843  \n",
       "17            (1, 3)         0.706554  \n",
       "18            (1, 1)         0.843214  \n",
       "19            (1, 2)         0.825920  \n",
       "20            (1, 3)         0.812505  \n",
       "21            (1, 1)         0.846069  \n",
       "22            (1, 2)         0.856576  \n",
       "23            (1, 3)         0.851189  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame({\n",
    "    'params': gs.cv_results_['params'],\n",
    "    #'param_ngram_range': gs.cv_results_['params']['vect__ngram_range'],\n",
    "    'mean_test_score': gs.cv_results_['mean_test_score']\n",
    "})\n",
    "tfidf_df = pd.json_normalize(tfidf_df['params']).join(tfidf_df['mean_test_score'])\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b9e4410-28fa-452e-8c09-ec12da206081",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf__fit_prior' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mtype\u001b[39m(clf__fit_prior)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clf__fit_prior' is not defined"
     ]
    }
   ],
   "source": [
    "type(clf__fit_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1308ce-40e5-49a5-b675-08c69d7de79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.getargspec(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e3955f-88dd-487a-93a7-2eb4db8be21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_estimator_._final_estimator.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84205d-8f04-4a4e-9337-ebb846118338",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    'word': gs.best_estimator_[:-1].get_feature_names_out(),\n",
    "    #'word': vect.get_feature_names_out(),\n",
    "    'coef': gs.best_estimator_._final_estimator.coef_[0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a83dcb0-b4b0-4005-b0aa-e9c3ea2d248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(gs.best_estimator_)\n",
    "explanation = explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea14fa-943e-4127-b66a-b77af653399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_estimator_.get_feature_names_out()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
